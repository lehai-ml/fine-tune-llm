{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLanCvs9VtPscQOhqTeXDd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lehai-ml/fine-tune-llm/blob/main/fine-tune-llm-unsloth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning LLM"
      ],
      "metadata": {
        "id": "WbkefpHWTka4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs currently have billions of parameters. I'm interested in understanding how you can fine-tune the LLM to fit a specific task. Fine-tuning an LLM from scratch is resource-intensive, and if not done appropriately, it could lead to the model losing its base language understanding.\n",
        "\n",
        "Parameter-efficient finetuning (PEFT) is a class of fine-tuning method. Low-Rank Approximation (LoRA) is a type of PEFT method that can achieve with minimal computational resources. PEFT methods can be defined into three subtypes [[1]](https://medium.com/@mujahidabdullahi1992/an-introduction-to-lora-unpacking-the-theory-and-practical-implementation-e665c5d78295):\n",
        "1. Selective - only a subset of weights are fine-tuned\n",
        "2. Reparametrisation - creates a low-dimensional representation of a specific module in the original LLM\n",
        "3. Additive - adding a new modules for fine-tuning. These modules are further trained to incorporate knowledge of the new domain into the pre-trained LLM."
      ],
      "metadata": {
        "id": "00snn_6cVl4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lower Rank Approximation method\n",
        "\n",
        "Put simply, previous researches found that a trained LLM to contain many redundant parameters and can function just as good with less weights (referred to as \"intrinsic rank\").\n",
        "\n",
        "If the weight matrix update is $W$, then it can be represented by two lower rank matrices, $A$ and $B$, such that $A$ is the lower dimension matrix and $B$ is the linear transformation of $A$ to the higher dimension, i.e.\n",
        "$\\Delta W = AB$\n",
        "\n",
        "Case in point, if you have 500 x 400 matrix, then it means you have 200000 parameters, but if we can decompose it into two lower rank matrices of 500 x 4 & 4 x 100, we only have to fine tune 2400 parameters.\n",
        "\n",
        "In terms of which weights to tune (and which to freeze) and what rank to set, the authors found that applying LoRA on several attention weights with a low rank can perform better than applying LoRA on a single attention weight with a high rank [[2]](https://arxiv.org/pdf/2106.09685).\n",
        "\n",
        "LoRA can be used in several context:\n",
        "1. Text classification - adpating a model to classify text into predefined categories, such as sentiment analysis or topic classification\n",
        "2. Question Answering: Fine-tuning the model to provide accurate answers to questions based on a given context or dataset\n",
        "3. Named Entity Recognition: Customising the model to identify and classify entities (like names, dates, locations) in text.\n",
        "4. Translation: improving the translation capabilities\n",
        "5. Dialogue systems: tailoring the response / output of the chatbots."
      ],
      "metadata": {
        "id": "DdbLMSljXzy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question and Answering\n",
        "\n",
        "Let's start with the BERT question and answering model. The point of this model is given a passage of text, the user can ask a question and can expect an highly accurate answer based on that text.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EyTlV-zcjZir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_model = pipeline(\"question-answering\")\n",
        "question = \"Where do I live?\"\n",
        "context = \"My name is Merve and I live in İstanbul.\"\n",
        "qa_model(question = question, context = context)\n",
        "## {'answer': 'İstanbul', 'end': 39, 'score': 0.953, 'start': 31}\n"
      ],
      "metadata": {
        "id": "OnNAFxSh3f9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "weight_path = \"kaporter/bert-base-uncased-finetuned-squad\"\n",
        "# loading tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(weight_path)\n",
        "#loading the model\n",
        "model = BertForQuestionAnswering.from_pretrained(weight_path)\n"
      ],
      "metadata": {
        "id": "mvWSCoEZ2i8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel, PeftConfig, LoraConfig, TaskType, get_peft_model\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "\n",
        "\n",
        "model= AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", id2label=id2label, label2id=label2id)\n",
        "dataset = load_dataset(\"rotten_tomatoes\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "cY3MSAqCWRhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wEbTxGDdWQki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Os8GhfaWVlLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R91LUXkOoycm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nWgvXna_pbUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Saving model\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "RiFMY7uTeNWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "eUoXceqJqzUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\")\n"
      ],
      "metadata": {
        "id": "M3858uOsok5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset['train']"
      ],
      "metadata": {
        "id": "TUUjjMnLq14J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset['train'].to_pandas()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NLmBwxS-rBdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Context_length'] = data['Context'].apply(len)\n",
        "plt.figure(figsize=(10, 3))\n",
        "sns.histplot(data['Context_length'], bins=50, kde=True)\n",
        "plt.title('Distribution of Context Lengths')\n",
        "plt.xlabel('Length of Context')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "aJTYvOcuq5aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = data[data['Context_length'] <= 500]\n",
        "\n",
        "ln_Context = filtered_data['Context'].apply(len)\n",
        "plt.figure(figsize=(10, 3))\n",
        "sns.histplot(ln_Context, bins=50, kde=True)\n",
        "plt.title('Distribution of Context Lengths')\n",
        "plt.xlabel('Length of Context')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q94xGMGnrAPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ln_Response = filtered_data['Response'].apply(len)\n",
        "plt.figure(figsize=(10, 3))\n",
        "sns.histplot(ln_Response, bins=50, kde=True, color='teal')\n",
        "plt.title('Distribution of Response Lengths')\n",
        "plt.xlabel('Length of Response')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W_reFS4Crd0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = filtered_data[ln_Response <= 1000]\n",
        "\n",
        "ln_Response = filtered_data['Response'].apply(len)\n",
        "plt.figure(figsize=(10, 3))\n",
        "sns.histplot(ln_Response, bins=50, kde=True, color='teal')\n",
        "plt.title('Distribution of Response Lengths')\n",
        "plt.xlabel('Length of Response')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CH0Z43ALrgy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data_sampled = filtered_data.sample(100)"
      ],
      "metadata": {
        "id": "HpRxXKYxrjdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 5020\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state = 32,\n",
        "    loftq_config = None,\n",
        ")\n",
        "print(model.print_trainable_parameters())\n"
      ],
      "metadata": {
        "id": "-g8AQf3krxV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prompt = \"\"\"Analyze the provided text from a mental health perspective. Identify any indicators of emotional distress, coping mechanisms, or psychological well-being. Highlight any potential concerns or positive aspects related to mental health, and provide a brief explanation for each observation.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompt(examples):\n",
        "    inputs       = examples[\"Context\"]\n",
        "    outputs      = examples[\"Response\"]\n",
        "    texts = []\n",
        "    for input_, output in zip(inputs, outputs):\n",
        "        text = data_prompt.format(input_, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n"
      ],
      "metadata": {
        "id": "VEaqGMS8r-vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = Dataset.from_pandas(filtered_data_sampled)\n",
        "training_data = training_data.map(formatting_prompt, batched=True)\n"
      ],
      "metadata": {
        "id": "K5lzh_8ssMBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ],
      "metadata": {
        "id": "wfbc07AMsOzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "0aev_2cMsX45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data"
      ],
      "metadata": {
        "id": "_qbsKhtKsqZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=training_data,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=3e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        num_train_epochs=10,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "hjIhzehmscKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I'm going through some things with my feelings and myself. \\\n",
        "I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. \\\n",
        "I've never tried or contemplated suicide. \\\n",
        "I've always wanted to fix my issues, but I never get around to it. \\\n",
        "How can I change my feeling of being worthless to everyone?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "XWY39oAgyMsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    data_prompt.format(\n",
        "        #instructions\n",
        "        text,\n",
        "        #answer\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 5020, use_cache = True)\n",
        "answer=tokenizer.batch_decode(outputs)\n",
        "answer = answer[0].split(\"### Response:\")[-1]\n",
        "print(\"Answer of the question is:\", answer)\n"
      ],
      "metadata": {
        "id": "ENu_xJrJsiOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "inlnR1KjyGiT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}